{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eECvcBsnoukK"
      },
      "source": [
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z_tpx13oukN"
      },
      "source": [
        "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Bayesian models for Machine Learning<br>Naive Bayes Classification</div>\n",
        "\n",
        "One very common application of naive Bayes classifiers is document classification (e-mail spam filtering, sentiment analysis on social networks, technical documentation classification, customer appreciations, etc.).\n",
        "\n",
        "Naive Bayes classifiers for documents estimate the probability of a given document belonging to a certain class Y of documents, based on the document's contents Xi.\n",
        "\n",
        "\n",
        "Suppose we want to predict the probability that sample $x$ has label $y$. This is a probability estimation problem that can be written:\n",
        "$$\\mathbb{P}(Y=y|X=x)$$\n",
        "\n",
        "According to Bayes' theorem, we have:\n",
        "$$\\mathbb{P}(Y=y|X=x) =\\frac{\\mathbb{P}(X=x|Y=y)\\cdot\\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}$$\n",
        "$$\\textrm{posterior} = \\frac{\\textrm{likelihood}\\cdot\\textrm{prior}}{\\textrm{evidence}}$$\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    \n",
        "**Bayesian inference** is the problem of estimating this **posterior distribution**.<br>\n",
        "In plain words, it consists in estimating the probability of label $y$, given an input $x$, using previously seen data to estimate the **likelihood** of an $x$ input associated to label $y$ and the general **prior** probability of observing label $y$.\n",
        "</div>\n",
        "\n",
        "Note that Bayesian inference applies both to classification and regression.\n",
        "\n",
        "The goal of Bayesian inference is to estimate the label distribution for a given $x$ and use them to predict the correct label, so it is a *probabilistic approach to Machine Learning*.\n",
        "\n",
        "The Bayesian predictor (classifier or regressor) returns the label that maximizes the posterior probability distribution.\n",
        "\n",
        "In this (first) notebook on Bayesian modeling in ML, we will explore the method of Naive Bayes Classification.\n",
        "\n",
        "1. [The naive Bayes assumption](#sec1)\n",
        "2. [Naive Bayes classifiers in scikit-learn](#sec2)\n",
        "3. [Examples](#sec3)\n",
        "    1. [The \"spam or ham?\" example](#sec3-1)\n",
        "    2. [The NIST example](#sec3-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aRtvRGSoukO"
      },
      "source": [
        "# 1. <a id=\"sec1\"></a>The naive Bayes assumption\n",
        "\n",
        "Let's start with some illustrative data. We consider an artificial data set of 9 individuals. The first column in our data set is the sex ($S=0$ for male, 1 for female), the second is the height $H$ (in meters), the third is the weight $W$ (in kilos) and the last is the foot size $F$ (in centimeters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z5mDcbnfoukO",
        "outputId": "5bd5d9ed-55c1-4c80-cd19-d7f841797d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "sex_classif.csv not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-141773896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# sex, height[m], weight[kg], foot size[cm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sex_classif.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1382\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: sex_classif.csv not found."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "scipdf = scipy.stats.norm.pdf\n",
        "fig_size=(10, 10)\n",
        "\n",
        "# sex, height[m], weight[kg], foot size[cm]\n",
        "data = np.loadtxt(\"sex_classif.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLpiwSjZoukQ"
      },
      "source": [
        "**Questions** :\n",
        "- Using matplotlib, bokeh, seaborn or plotly, plot one relevant figure on the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jBEl2VqoukQ"
      },
      "outputs": [],
      "source": [
        "# your plot here\n",
        "plt.figure()\n",
        "plt.hist(data[:, 0], label=\"sex\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(data[:, 1], label=\"height\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(data[:, 2], label=\"weight\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(data[:, 3], label=\"foot size\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(data[:, [0, 1]], cmap='hot', interpolation='nearest')\n",
        "plt.title(\"sex vs height\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEI9ZhMfoukQ"
      },
      "source": [
        "We would like to answer the question: is $(H=1.81, W=59, F=21)$ male or female?\n",
        "\n",
        "Let's try to estimate $\\mathbb{P}(S=0|H=1.81, W=59, F=21)$.\n",
        "\n",
        "According to Bayes' theorem, the probability that a person that measures 1.81m, weights 59kgs and has a foot size of 21cm is male, is actually the likelihood of observing a person with such features among males, multiplied by the probability of observing males in the population, divided by the probability of observing an individual with these features.\n",
        "\n",
        "That's a long sentence. Let's write that mathematically:\n",
        "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81, W=59, F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
        "\n",
        "Let's make that more readable and more general:\n",
        "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H,W,F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H,W,F)}$$\n",
        "\n",
        "Interestingly, since our goal is only to compare the probabilities for $S=0$ and $S=1$, the denominator in the last equation won't be relevant. So we are left with two terms to estimate, given the available data:\n",
        "- $\\mathbb{P}(S=0)$: the prior - the probability that any individual is $S=0$, regardless of his/her physical attributes;\n",
        "- $\\mathbb{P}(H=1.81, W=59, F=21 | S=0)$: the likelihood of meeting somebody with the specified features, given that his/her sex is $S=0$.\n",
        "\n",
        "The prior, in this case, is easy to estimate by comparing the frequencies of male and female individuals in the population.\n",
        "\\begin{gather*}\n",
        "\\mathbb{P}(S=0) = \\frac{4}{9}\\\\\n",
        "\\mathbb{P}(S=1) = \\frac{5}{9}\n",
        "\\end{gather*}\n",
        "Technically, the estimate above is obtained by [*maximum likelihood estimation*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
        "\n",
        "The likelihood, however, is a bit trickier. Can we directly estimate the **joint probability** of the 3 variables $(H,W,F)$?\n",
        "\n",
        "Theoretically, we can. We can assume that among male individuals, $(H,W,F)$ are distributed according to a multivariate Normal distribution, with mean $\\mu=(\\mu_H, \\mu_W, \\mu_F)$ and covariance matrix $\\Sigma$. The trick is then to estimate $\\mu$ and $\\Sigma$.\n",
        "\n",
        "As a matter of fact, estimating $\\mu$ and $\\Sigma$ without further hypothesis would require quite a lot of data, especially because $\\Sigma$ captures the **correlation** between $H$, $W$ and $F$.\n",
        "\n",
        "$\\Sigma$ is a $3\\times 3$ matrix, so it involves 9 parameters to estimate, and we unfortunately only have 9 data points.\n",
        "\n",
        "Let's rephrase this from another perspective. With some basic probabilities, we have:\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
        "& \\cdot \\mathbb{P}(W | S, H) \\\\\n",
        "& \\cdot \\mathbb{P}(F | S, H, W)\n",
        "\\end{align*}\n",
        "\n",
        "Those three probabilities are univariate probabilities, much easier to estimate. However, the first one is a function of $S$ only, the second one depends on $S$ and $H$ and the third one depends on $S$, $H$ and $W$. To get an accurate estimate of the third one, we would need samples of the distribution of $F$ in enough points in the space of $(S,H,W)$ to cover it reasonably. This would require a number of data points that is exponential in the number of variables. That's what is called the **curse of dimensionality**, which makes this estimation problem difficult.\n",
        "\n",
        "Let's make this concrete. Suppose we discretize $H$, $W$ and $F$ in 10 bins each and suppose we require 100 samples to get a correct estimate of $\\mathbb{P}(F | S, H, W)$ for any given value of $(F, S, H, W)$. Then we need $100\\cdot 10^3\\cdot 2$ samples to correctly estimate this probability for all possible values of $(F, S, H, W)$. More generally, if we had $n$ continuous features rather than just three, we would require a number of data points that is exponential in $n$.\n",
        "\n",
        "To circumvent this problem, we are going to make a very **naive** assumption (hence the name of the method). We are going to assume that the weight, the height and the foot size are totally independent variables, that is the probability that a person be 1.85m is the same whatever his/her weight and foot size.\n",
        "\n",
        "Obviously, this hypothesis is very strong and clearly does not hold is most real-world cases. But we will assume it nonetheless. In this case, the likelihood estimation becomes:\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(H,W,F | S) = &\\mathbb{P}(H | S)\\\\\n",
        "& \\cdot \\mathbb{P}(W | S) \\\\\n",
        "& \\cdot \\mathbb{P}(F | S)\n",
        "\\end{align*}\n",
        "\n",
        "Each of these probabilities now only depends on the label $S$ and is much easier to estimate from the data. This **conditional independence** assumption is called the **naive Bayes hypothesis**. It allow us to give a (very bad) estimate of $\\mathbb{P}(X | Y)$ and hence of $\\mathbb{P}(Y|X)$.\n",
        "\n",
        "$$\\mathbb{P}(S|H, W, F) = \\frac{\\mathbb{P}(H | S)\\cdot \\mathbb{P}(W | S) \\cdot \\mathbb{P}(F | S)\\cdot \\mathbb{P}(S)}{\\mathbb{P}(H, W, F)}$$\n",
        "\n",
        "Or, in our case:\n",
        "\n",
        "$$\\mathbb{P}(S=0|H=1.81, W=59, F=21) = \\frac{\\mathbb{P}(H=1.81 | S=0)\\cdot \\mathbb{P}(W=59 | S=0) \\cdot \\mathbb{P}(F=21 | S=0)\\cdot \\mathbb{P}(S=0)}{\\mathbb{P}(H=1.81, W=59, F=21)}$$\n",
        "\n",
        "The **naive Bayes classifier** is then the classifier that estimates all class probabilities and returns the one with maximum probability.\n",
        "\n",
        "$$f(H, W, F) = \\arg\\max_{s} \\mathbb{P}(S=s|H,W,F) = \\arg\\max_{s} \\mathbb{P}(H|S=s)\\cdot \\mathbb{P}(W|S=s) \\cdot \\mathbb{P}(F|S=s)\\cdot \\mathbb{P}(S=s)$$\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "    \n",
        "**Exercice:**<br>\n",
        "Let's implement a naive Bayes classifier on the data above, just to practice. We will assume that the $\\mathbb{P}(X | S)$ distributions are Gaussians (for $X = H,W,$ or $F$). Compute the scores and probabilities for each sex, for $(H=1.81, W=59, F=21)$.<br>\n",
        "Hint: use the `np.mean` and `np.std` functions to estimate distribution parameters. Use `scipy.stats.norm.pdf` to compute the Gaussian probability density function in a given input.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfG05SZaoukR"
      },
      "outputs": [],
      "source": [
        "# P(H | S=0)\n",
        "dataM = data[data[:, 0] == 0]\n",
        "dataF = data[data[:, 0] == 1]\n",
        "\n",
        "# values of height\n",
        "vals_H = np.linspace(np.min(data[:, 1]*0.9), np.max(data[:, 1]*1.1), 100)\n",
        "# extrapolate distribution\n",
        "PDF_M_H = scipy.stats.norm(np.mean(dataM[:, 1]), np.std(dataM[:, 1]))\n",
        "\n",
        "\n",
        "# values of weight\n",
        "vals_W = np.linspace(np.min(data[:, 2]*0.9), np.max(data[:, 2]*1.1), 100)\n",
        "# extrapolate distribution\n",
        "PDF_M_W = scipy.stats.norm(np.mean(dataM[:, 2]), np.std(dataM[:, 2]))\n",
        "\n",
        "\n",
        "\n",
        "# values of foot size\n",
        "vals_F = np.linspace(np.min(data[:, 3]*0.9), np.max(data[:, 3]*1.1), 100)\n",
        "# extrapolate distribution\n",
        "PDF_M_F = scipy.stats.norm(np.mean(dataM[:, 3]), np.std(dataM[:, 3]))\n",
        "\n",
        "\n",
        "\n",
        "# idem for women\n",
        "PDF_F_H = scipy.stats.norm(np.mean(dataF[:, 1]), np.std(dataF[:, 1]))\n",
        "PDF_F_W = scipy.stats.norm(np.mean(dataF[:, 2]), np.std(dataF[:, 2]))\n",
        "PDF_F_F = scipy.stats.norm(np.mean(dataF[:, 3]), np.std(dataF[:, 3]))\n",
        "\n",
        "\n",
        "\n",
        "# evaluate and plot results\n",
        "plt.figure()\n",
        "plt.plot(vals_H, PDF_M_H.pdf(vals_H), label=\"extrapolated height dist for men\", color=\"dodgerblue\")\n",
        "plt.plot(vals_H, PDF_F_H.pdf(vals_H), label=\"extrapolated height dist for women\", color=\"pink\")\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2mHRT-AoukR"
      },
      "outputs": [],
      "source": [
        "# %load solutions/code1.py\n",
        "### WRITE YOUR CODE HERE\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5iIPGvkoukS"
      },
      "source": [
        "It appears we will always multiply together values that are smaller than one. The result will quickly become very small. It is a good habit to move to log-scale.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "**Exercice:**<br>\n",
        "Reuse your code above to compute log scores instead of scores.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgW6eQjcoukS"
      },
      "outputs": [],
      "source": [
        "# idem, avec log scores\n",
        "# au lieu de multiplier les probas, on somme les logs de probas\n",
        "\n",
        "\n",
        "# evaluate and log\n",
        "PDF_M_H_log = np.log(PDF_M_H.pdf(1.81))\n",
        "PDF_M_W_log = np.log(PDF_M_W.pdf(59))\n",
        "PDF_M_F_log = np.log(PDF_M_F.pdf(21))\n",
        "\n",
        "# to compute formula, we need P(S=0) and P(H, W, F)\n",
        "PS_M = np.sum(data[:, 0] == 0) / len(data[:, 0])\n",
        "print(\"P(male) : \", PS_M)\n",
        "PS_M_log = np.log(PS_M)\n",
        "PS_F_log = np.log(1 - PS_M)\n",
        "\n",
        "# BUT we do not need P(H, W, F) : we just want to compare the scode\n",
        "score_M = PS_M_log + PDF_M_H_log + PDF_M_W_log + PDF_M_F_log\n",
        "print(f\"score men : {score_M}\")\n",
        "\n",
        "\n",
        "# avaluate and log hypothesis women\n",
        "PDF_F_H_log = np.log(PDF_F_H.pdf(1.81))\n",
        "PDF_F_W_log = np.log(PDF_F_W.pdf(59))\n",
        "PDF_F_F_log = np.log(PDF_F_F.pdf(21))\n",
        "\n",
        "score_F = PS_F_log + PDF_F_H_log + PDF_F_W_log + PDF_F_F_log\n",
        "print(f\"score women : {score_F}\")\n",
        "\n",
        "# c'est une femme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrXF7OBdoukS"
      },
      "outputs": [],
      "source": [
        "# %load solutions/code2.py\n",
        "### WRITE YOUR CODE HERE\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZJAO-hKoukS"
      },
      "source": [
        "Conclusion: $(H=1.81,W=59,F=21)$ is most probably female.\n",
        "\n",
        "Let's generalize.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "    \n",
        "    \n",
        "Given $n$ features $X_i$ and classes $Y$, **naive Bayes classifiers** estimate (from data) the distributions $\\mathbb{P}(Y)$ and $\\mathbb{P}(X_i|Y)$. Then, using Bayes rule and the naive Bayes assumption, they predict the most probable estimated class:\n",
        "\\begin{align*}\n",
        "\\arg\\max_{y} \\mathbb{P}(Y=y|X=x) & = \\arg\\max_{y} \\frac{\\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)}{\\mathbb{P}(X=x)}\\\\\n",
        "& = \\arg\\max_{y} \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\\\\\n",
        "& = \\arg\\max_{y} \\sum\\limits_{i=1}^n \\log\\left(\\mathbb{P}(X_i=x_i|Y=y)\\right) + \\log\\left(\\mathbb{P}(Y=y)\\right)\n",
        "\\end{align*}\n",
        "</div>\n",
        "\n",
        "Note that although it is not compulsory to compute the denominator, it is quite straightforward since:\n",
        "\\begin{align*}\n",
        "\\mathbb{P}(X=x) &= \\sum\\limits_y \\mathbb{P}(X=x|Y=y)\\mathbb{P}(Y=y)\\\\\n",
        "&= \\sum\\limits_y \\prod\\limits_{i=1}^n \\mathbb{P}(X_i=x_i|Y=y) \\mathbb{P}(Y=y)\n",
        "\\end{align*}\n",
        "So it's the sum of the numerator's values for all $y$, so it's just a matter of normalizing the scores obtained.\n",
        "\n",
        "A really nice thing about naive Bayes classifiers is that it is an **online method**, since most univariate probability distributions can be updated incrementally,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ADioEe4oukS"
      },
      "source": [
        "# <a id=\"sec2\"></a> 2. Naive Bayes classifiers in scikit-learn\n",
        "\n",
        "Once again, scikit-learn has a [naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) implementation. It allows three kind of distributions for the $X_i|Y$ variables: Normal (continuous), Bernouilli or Multinomial (discrete).\n",
        "Let's directly use it on our toy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8WjEJgAoukT"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "\n",
        "X = data[:,1:] # weight, height, footsize\n",
        "y = data[:,0] # sex\n",
        "\n",
        "gnb.fit(X,y)\n",
        "\n",
        "xtest = np.array([[1.81,59,21]])\n",
        "\n",
        "print(\"Prediction: \", gnb.predict(xtest))\n",
        "print(\"Probas:     \", gnb.predict_proba(xtest))\n",
        "print(\"Log probas: \",gnb.predict_log_proba(xtest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YmQuuc8oukT"
      },
      "source": [
        "# <a id=\"sec3\"></a> 3. Examples\n",
        "\n",
        "## <a id=\"sec3-1\"></a> 3.1 The \"spam or ham?\" example\n",
        "\n",
        "Let's scale up and apply naive Bayes classification on the ling-spam data. We will assume a multinomial distribution of word $i$ appearing in and email of class $y$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlbx5LrUoukT"
      },
      "outputs": [],
      "source": [
        "from sys import path\n",
        "path.append('../2 - Text data preprocessing')\n",
        "import load_spam\n",
        "spam_data = load_spam.spam_data_loader()\n",
        "spam_data.load_data()\n",
        "print(\"data loaded\")\n",
        "\n",
        "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jojY5-EeoukT"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    \n",
        "**Exercice:**\n",
        "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the data above. Estimate its generalization error.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my3AR52SoukT"
      },
      "outputs": [],
      "source": [
        "# %load solutions/code3.py\n",
        "### WRITE YOUR CODE HERE\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU41IsAUoukU"
      },
      "source": [
        "We've trained our model in the Tf-Idf data. Let's see how the model behaves on raw word counts.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "**Exercice:**\n",
        "Use scikit-learn to build a [multinomial naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) on the raw word counts data below. Estimate its generalization error.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_MXASlzoukU"
      },
      "outputs": [],
      "source": [
        "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZGGZGlCoukU"
      },
      "outputs": [],
      "source": [
        "# %load solutions/code4.py\n",
        "### WRITE YOUR CODE HERE\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8YwRRAToukU"
      },
      "source": [
        "Let's identify which are the misclassified emails (and find the confusion matrix by the way)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkvDQoHqoukU"
      },
      "outputs": [],
      "source": [
        "# Retrain\n",
        "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')\n",
        "spam_nbc = MultinomialNB()\n",
        "spam_nbc.fit(Xtrain,ytrain);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm_eWVP4oukU"
      },
      "outputs": [],
      "source": [
        "# Find misclassified examples\n",
        "ypredict = spam_nbc.predict(Xtest)\n",
        "misclass = np.not_equal(ypredict, ytest)\n",
        "Xmisclass = Xtest[misclass,:]\n",
        "ymisclass = ytest[misclass]\n",
        "misclass_indices = [i for i, j in enumerate(misclass) if j == True]\n",
        "print(\"Misclassified messages indices:\", misclass_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUesd4s7oukU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(ytest, ypredict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4bXVrRE-oukU"
      },
      "outputs": [],
      "source": [
        "# Check some misclassified mails\n",
        "index = misclass_indices[1]+2000\n",
        "print(\"Prediction:\", spam_nbc.predict(spam_data.word_count[index,:]))\n",
        "spam_data.print_email(index)\n",
        "spam_nbc.predict_proba(spam_data.tfidf[index,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LszWtzyRoukU"
      },
      "source": [
        "**Questions** :\n",
        "- What are the next steps to improve the results ? To create the moderation system ?\n",
        "- What questions should you ask to the tech company, before working on their data ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb29U_eToukU"
      },
      "source": [
        "## <a id=\"sec3-2\"></a> 3.2 The NIST example\n",
        "\n",
        "We will assume Gaussian distributions for the NIST example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpk3XTqwoukV"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "digits = datasets.load_digits()\n",
        "print(digits.data.shape)\n",
        "print(digits.images.shape)\n",
        "print(digits.target.shape)\n",
        "print(digits.target_names)\n",
        "\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "Xtrain,Xtest = np.split(X,[1000])\n",
        "ytrain,ytest = np.split(y,[1000])\n",
        "#Xtrain = X[:1000,:]\n",
        "#ytrain = y[:1000]\n",
        "#Xtest = X[1000:,:]\n",
        "#ytest = y[1000:]\n",
        "\n",
        "#print(digits.DESCR)\n",
        "\n",
        "#plt.gray();\n",
        "#plt.matshow(digits.images[0]);\n",
        "#plt.show();\n",
        "#plt.matshow(digits.images[15]);\n",
        "#plt.show();\n",
        "#plt.matshow(digits.images[42]);\n",
        "#plt.show();\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def shuffle_and_split(X,y,n):\n",
        "    X0,y0 = shuffle(X,y)\n",
        "    Xtrain,Xtest = np.split(X0,[n])\n",
        "    ytrain,ytest = np.split(y0,[n])\n",
        "    return Xtrain, ytrain, Xtest, ytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-VURyR_oukV"
      },
      "outputs": [],
      "source": [
        "Xtrain, ytrain, Xtest, ytest  = shuffle_and_split(X,y,1000)\n",
        "\n",
        "print(Xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "digits_nbc = GaussianNB()\n",
        "digits_nbc.fit(Xtrain,ytrain)\n",
        "prediction = digits_nbc.predict(Xtest)\n",
        "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
        "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
        "print(\"Generalization score:\", digits_nbc.score(Xtest,ytest))\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(ytest, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5-iGVPGoukV"
      },
      "outputs": [],
      "source": [
        "# Compute cross-validation score\n",
        "nb_trials = 20\n",
        "score = []\n",
        "for i in range(nb_trials):\n",
        "    Xtrain, ytrain, Xtest, ytest = shuffle_and_split(X,y,1000)\n",
        "    digits_nbc = GaussianNB()\n",
        "    digits_nbc.fit(Xtrain,ytrain)\n",
        "    score += [digits_nbc.score(Xtest,ytest)]\n",
        "    print('*',end='')\n",
        "print(\" done!\")\n",
        "\n",
        "print(\"Average generalization score:\", np.mean(score))\n",
        "print(\"Standard deviation:\", np.std(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dcNs_pUoukV"
      },
      "source": [
        "Naive Bayes classifiers reach their limits on data with high correlations between features (like images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXparNIUoukV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".sdd_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": false,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}